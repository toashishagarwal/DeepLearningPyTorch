{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch trl peft bitsandbytes"
      ],
      "metadata": {
        "id": "AioHwz9egQCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load required libraries\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "def generate_text(prompt, max_length=500, temperature=0.1):\n",
        "    \"\"\"\n",
        "    Generate text using the deepseek-r1\n",
        "\n",
        "    Args:\n",
        "        prompt (str): Input text to generate from\n",
        "        max_length (int): Maximum length of generated text\n",
        "        temperature (float): Controls randomness in generation (0.0-1.0)\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text\n",
        "    \"\"\"\n",
        "    # Encode the input text\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate text\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example prompts to test the model\n",
        "    prompts = [\n",
        "        \"In Yoga philosophy, what is the significance of the concept of ahimsa (non-violence)?\",\n",
        "        \"Tell me about buddhism in India?\"\n",
        "    ]\n",
        "\n",
        "    print(\"Generating text from different prompts:\\n\")\n",
        "    for prompt in prompts:\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        generated = generate_text(prompt)\n",
        "        print(f\"Generated text: {generated}\\n\")"
      ],
      "metadata": {
        "id": "KEFkd7k9gW_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "dataset = load_dataset(\"Abhaykoul/Ancient-Indian-Wisdom\")\n",
        "\n",
        "# Step 2: Format the dataset into instruction-response pairs\n",
        "def format_dataset(examples):\n",
        "    \"\"\"Format the dataset into instruction-response pairs.\"\"\"\n",
        "    texts = []\n",
        "    for instruction, response in zip(examples[\"instruction\"], examples[\"output\"]):\n",
        "        # Combine instruction and response into a single text\n",
        "        formatted_text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n",
        "        texts.append(formatted_text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply formatting\n",
        "dataset = dataset.map(format_dataset, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "# Step 3: Load model and tokenizer\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# Step 4: Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    r=16,                   # Rank of the low-rank matrices\n",
        "    lora_alpha=32,          # Scaling factor\n",
        "    lora_dropout=0.1,       # Dropout for LoRA layers\n",
        "    bias=\"none\",            # No bias for LoRA\n",
        "    task_type=\"CAUSAL_LM\",  # Task type\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Target modules for LoRA\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Step 5: Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",             # Directory to save results\n",
        "    num_train_epochs=200,               # Number of training epochs\n",
        "    per_device_train_batch_size=4,      # Batch size per device\n",
        "    per_device_eval_batch_size=4,       # Evaluation batch size\n",
        "    gradient_accumulation_steps=4,      # Gradient accumulation steps\n",
        "    gradient_checkpointing=False,       # Disable gradient checkpointing for debugging\n",
        "    optim=\"adamw_torch\",                # Optimizer\n",
        "    learning_rate=1e-4,                 # Learning rate\n",
        "    warmup_ratio=0.1,                   # Warmup ratio\n",
        "    fp16=True,                          # Use mixed precision (FP16)\n",
        "    logging_steps=10,                   # Log every 10 steps\n",
        "    save_strategy=\"steps\",              # Save model at specific steps\n",
        "    save_steps=100,                     # Save every 100 steps\n",
        "    eval_strategy=\"steps\",              # Evaluate at specific steps\n",
        "    eval_steps=100,                     # Evaluate every 100 steps\n",
        "    eval_accumulation_steps=1,          # Accumulate evaluation steps\n",
        "    load_best_model_at_end=True,        # Load the best model at the end\n",
        "    metric_for_best_model=\"eval_loss\",  # Metric for best model\n",
        "    greater_is_better=False,            # Lower eval_loss is better\n",
        "    remove_unused_columns=True,         # Remove unused columns\n",
        "    report_to=\"none\",                   # Disable external logging\n",
        ")\n",
        "\n",
        "# Step 6: Initialize the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"train\"].select(range(120)),  # Small evaluation set\n",
        "    #tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Step 7: Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "BCVFuc3Pkkj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"fine-tuned-deepseek-r1-1.5b\")\n",
        "tokenizer.save_pretrained(\"fine-tuned-deepseek-r1-1.5b\")"
      ],
      "metadata": {
        "id": "RFCHhUaftOnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_path = \"fine-tuned-deepseek-r1-1.5b\"\n",
        "\n",
        "# Load model with optimizations\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "def generate_text(prompt, max_new_tokens=1000):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.5,\n",
        "            top_k=50,\n",
        "            top_p=0.9,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Test\n",
        "prompt = \"In Yoga philosophy, what is the significance of the concept of ahimsa (non-violence)?\"\n",
        "output = generate_text(prompt)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "-FOLYmAD5TLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Current device: {torch.cuda.current_device()}\")\n",
        "print(f\"Device name: {torch.cuda.get_device_name()}\")"
      ],
      "metadata": {
        "id": "VQRfdieuSauH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}